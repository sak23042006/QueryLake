# LakeRAG Environment Configuration Template
# Copy this file to .env and fill in your actual values
# IMPORTANT: Never commit .env to version control!

# ============================================================================
# AWS CREDENTIALS
# ============================================================================
# AWS Access Key ID for S3 bucket access
# Get from: AWS Console → IAM → Users → Security Credentials
AWS_ACCESS_KEY_ID=AKIA...

# AWS Secret Access Key (keep this secret!)
AWS_SECRET_ACCESS_KEY=your_secret_access_key_here

# AWS Region where your S3 bucket is located
# Options: us-east-1, ap-south-1, eu-west-1, etc.
AWS_REGION=ap-south-1

# ============================================================================
# S3 BUCKET CONFIGURATION
# ============================================================================
# Name of your S3 bucket for data storage
# Must be globally unique and already created
BUCKET_NAME=lakerag-arun-bootcamp

# ============================================================================
# GOOGLE GEMINI API
# ============================================================================
# Gemini API key for AI summarization
# Get from: https://ai.google.dev/
GEMINI_API_KEY=AIza...

# ============================================================================
# POSTGRESQL DATABASE (for Airflow)
# ============================================================================
# PostgreSQL username
POSTGRES_USER=airflow

# PostgreSQL password
POSTGRES_PASSWORD=airflow

# PostgreSQL database name
POSTGRES_DB=airflow

# ============================================================================
# AIRFLOW CONFIGURATION
# ============================================================================
# Airflow admin username for web UI login
AIRFLOW_ADMIN_USERNAME=admin

# Airflow admin password for web UI login
AIRFLOW_ADMIN_PASSWORD=admin

# Airflow admin user's first name
AIRFLOW_ADMIN_FIRSTNAME=Admin

# Airflow admin user's last name
AIRFLOW_ADMIN_LASTNAME=User

# Airflow admin email address
AIRFLOW_ADMIN_EMAIL=admin@example.com

# ============================================================================
# AIRFLOW SECURITY KEYS
# ============================================================================
# Fernet key for encrypting passwords in Airflow database
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
FERNET_KEY=your_fernet_key_here

# Secret key for Flask sessions in Airflow webserver
# Generate with: openssl rand -hex 16
SECRET_KEY=your_secret_key_here

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
# Log level for all services
# Options: debug, info, warning, error, critical
LOG_LEVEL=info

# ============================================================================
# PYTHON OPTIMIZATIONS
# ============================================================================
# Disable Python bytecode (.pyc) files
PYTHONDONTWRITEBYTECODE=1

# Force unbuffered stdout/stderr (for real-time logs)
PYTHONUNBUFFERED=1

# ============================================================================
# DOCKER COMPOSE OVERRIDES (Optional)
# ============================================================================
# Compose project name (defaults to directory name)
# COMPOSE_PROJECT_NAME=lakerag

# ============================================================================
# BACKEND API CONFIGURATION (Optional)
# ============================================================================
# Backend API host (default: 0.0.0.0)
# BACKEND_HOST=0.0.0.0

# Backend API port (default: 8000)
# BACKEND_PORT=8000

# ============================================================================
# SPARK CONFIGURATION (Optional)
# ============================================================================
# Spark master memory (default: 2g)
# SPARK_MASTER_MEMORY=2g

# Spark worker memory (default: 4g)
# SPARK_WORKER_MEMORY=4g

# Spark worker cores (default: 4)
# SPARK_WORKER_CORES=4

# ============================================================================
# EMBEDDINGS MODEL CONFIGURATION (Optional)
# ============================================================================
# HuggingFace model for embeddings (default: BAAI/bge-large-en-v1.5)
# EMBEDDINGS_MODEL=BAAI/bge-large-en-v1.5

# Batch size for embeddings generation (default: 32)
# EMBEDDINGS_BATCH_SIZE=32

# ============================================================================
# FAISS INDEX CONFIGURATION (Optional)
# ============================================================================
# FAISS index type (default: IndexFlatIP)
# FAISS_INDEX_TYPE=IndexFlatIP

# Similarity threshold for search results (default: 0.65)
# SIMILARITY_THRESHOLD=0.65

# ============================================================================
# NOTES
# ============================================================================
# 1. Required variables (must be set):
#    - AWS_ACCESS_KEY_ID
#    - AWS_SECRET_ACCESS_KEY
#    - AWS_REGION
#    - BUCKET_NAME
#    - GEMINI_API_KEY
#    - FERNET_KEY (generate it!)
#    - SECRET_KEY (generate it!)
#
# 2. Generate security keys:
#    Fernet Key:
#      python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
#    
#    Secret Key:
#      openssl rand -hex 16
#
# 3. IAM Permissions needed for AWS user:
#    - s3:GetObject
#    - s3:PutObject
#    - s3:DeleteObject
#    - s3:ListBucket
#
# 4. Create S3 bucket before starting:
#    aws s3 mb s3://lakerag-arun-bootcamp --region ap-south-1
#
# 5. Keep this file secure:
#    - Add .env to .gitignore
#    - Never share your credentials
#    - Rotate keys regularly
#
# 6. For production deployment:
#    - Use AWS Secrets Manager or similar
#    - Use strong passwords
#    - Enable MFA on AWS account
#    - Use separate credentials for prod/dev
#
# ============================================================================
